{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "\n",
    "Loss function: \n",
    "$$\n",
    "L(\\mathbf{w} ; \\mathbf{x}, y) :=\\frac{1}{2}\\left(\\mathbf{w}^{T} \\mathbf{x}-y\\right)^{2}\n",
    "$$\n",
    "\n",
    "Ridge regression uses L2 regularization.\n",
    "\n",
    "Reference: [linear methods](https://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-least-squares-lasso-and-ridge-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle component analysis (PCA)\n",
    "\n",
    "Reference: [PCA](https://spark.apache.org/docs/2.3.0/mllib-dimensionality-reduction.html#principal-component-analysis-pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampled randomised Hadamard Transform\n",
    "\n",
    "$r = 500$\n",
    "\n",
    "We can first select $1024$ entries from the rdd, transformed it and then divide into training set and test set.\n",
    "\n",
    "Sampling [with/without replacement](https://web.ma.utexas.edu/users/parker/sampling/repl.htm)\n",
    "\n",
    "- takeSample() randomly selects data, with replacement\n",
    "- does the shuffle job. => P\n",
    "- Should we take with or without replacement?\n",
    "- When sampling from a given data, your are treating those data as the “true pool” of possible outcomes, thus we usually sample with replacement, ie. the pool should not alter or become smaller when you sample from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
    "from pyspark.mllib.feature import PCA\n",
    "from random import randrange\n",
    "from pyspark.mllib.linalg import DenseVector, DenseMatrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the data from csv\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(',')]\n",
    "    return LabeledPoint(values[-1], values[0:len(values)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression\n",
    "def rr_fit(parsed_Data):\n",
    "    rdd = parsed_Data.randomSplit([0.8, 0.2])\n",
    "    model = LinearRegressionWithSGD.train(rdd[0], iterations=100,\n",
    "                                          step=0.00000001, regType=\"l2\")\n",
    "\n",
    "    # Evaluate the model on training data\n",
    "    valuesAndPreds = rdd[1].map(lambda p: (p.label, model.predict(p.features)))\n",
    "    MSE = valuesAndPreds.map(lambda vp: (vp[0] - vp[1])**2)\\\n",
    "              .reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "    print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# principle component analysis\n",
    "def pca_fit(parsed_Data):\n",
    "    x = parsed_Data.map(lambda p: p.features)\n",
    "    pc = PCA(5).fit(x)\n",
    "    transformed = pc.transform(x)\n",
    "    y = parsed_Data.map(lambda p: p.label)\n",
    "    a = transformed.zip(y)\n",
    "    paired = a.map(lambda line: LabeledPoint(line[1], line[0]))\n",
    "\n",
    "    rdd2 = paired.randomSplit([0.8, 0.2])\n",
    "    model2 = LinearRegressionWithSGD.train(rdd2[0], iterations=100,\n",
    "                                           step=0.00000001, regType=None)\n",
    "\n",
    "    # Evaluate the model on training data\n",
    "    valuesAndPreds = rdd2[1].map(lambda p: (p.label, model2.predict(p.features)))\n",
    "    MSE = valuesAndPreds.map(lambda vp: (vp[0] - vp[1])**2)\\\n",
    "              .reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "    print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix D\n",
    "def mat_D(x):\n",
    "    d = randrange(-1, 2, 2)\n",
    "    return d*x\n",
    "\n",
    "# matrix B serves as filter\n",
    "def mat_B_filter(x):\n",
    "    d = randrange(0, 1024)\n",
    "    return d < 500\n",
    "\n",
    "# Hadamard transform\n",
    "def hadamard_fit(data):\n",
    "    # sample 1024 terms from data\n",
    "    parsedData = data.map(lambda line: np.array([float(x) for x in line.split(',')]))\n",
    "    rdd3 = sc.parallelize(parsedData.takeSample(True, 1024),2)\n",
    "\n",
    "    # create Hadamard matrix\n",
    "    N = 10\n",
    "    H = np.zeros([1024, 1024])\n",
    "    H[0, 0] = 1\n",
    "    h = 1\n",
    "    for i in range(N):\n",
    "        H[0:h, h:2 * h] = H[0:h, 0:h]\n",
    "        H[h:2 * h, 0:h] = H[0:h, 0:h]\n",
    "        H[h:2 * h, h:2 * h] = -1 * H[0:h, 0:h]\n",
    "        h = h * 2\n",
    "\n",
    "    # multiply with Hadamard matrix\n",
    "    lens = rdd3.collect()[0].shape[0]\n",
    "    X_array = np.array(rdd3.collect()).reshape(1024, lens)\n",
    "    X_hadamard = H.dot(X_array)\n",
    "\n",
    "    x_rdd = sc.parallelize(X_hadamard)  # each entry is an numpy array\n",
    "    subset = x_rdd.map(lambda x: LabeledPoint(x[-1], x[0:lens - 1])) \\\n",
    "        .randomSplit([0.8, 0.2])  # split training and testing\n",
    "    x_rp = subset[0].filter(mat_B_filter)  # mat B actually serve as a filter\n",
    "    model3 = LinearRegressionWithSGD.train(x_rp, iterations=100,\n",
    "                                           step=0.00000001, regType=None)\n",
    "    # Evaluate the model on training data\n",
    "    valuesAndPreds = subset[1].map(lambda p: (p.label, model3.predict(p.features)))\n",
    "    MSE = valuesAndPreds \\\n",
    "              .map(lambda vp: (vp[0] - vp[1]) ** 2) \\\n",
    "              .reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "    print(\"Mean Squared Error = \" + str(MSE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(\"pynum.csv\")\n",
    "parsedData = data.map(parsePoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 3522768811.3\n"
     ]
    }
   ],
   "source": [
    "rr_fit(parsedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 3561025567.59\n"
     ]
    }
   ],
   "source": [
    "pca_fit(parsedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = inf\n"
     ]
    }
   ],
   "source": [
    "hadamard_fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
